{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Picking \n",
    "## Group 1 ML Part II Coursework\n",
    "\n",
    "\n",
    "Stock picking has been a long practice in the market where mutual funds have clamied they can beat the market. The claim has been debated since the Princeton professor Burton Malkiel said expert's stock picks peform as well as blind monkeys throwing darts. Some have even demonstrated the monkey even did a better job than the experts.\n",
    "\n",
    "https://www.forbes.com/sites/rickferri/2012/12/20/any-monkey-can-beat-the-market/#24c34e92630a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we try to use Machine Learning classifier to beat the monkey's performance. Since it is illegal to own one, we simulate the monkey by a random picking algo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we structure the data. We collected stocks listed on NYSE and NASDAQ fundamental financial ratios of Q4 2017 as features(X) as well as price information of Q4 2017-18 from Bloomberg. We drop the Nan when Bloomberg has no avilable data. Bloomberg is the lead market participant in financial data provider. If they don't have the information, doubt other data vendor (FACTSET, Reuters) has it. We split the data by the function that exists in sklearn. The reason behind it is that the function probably handles splitting the data a lot better than we would have and it is better practice to not re-write and use already existing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>cash_ratio</th>\n",
       "      <th>return_to_equity</th>\n",
       "      <th>price_to_book</th>\n",
       "      <th>pe</th>\n",
       "      <th>short_interest_ratio</th>\n",
       "      <th>debt_to_equity</th>\n",
       "      <th>eps</th>\n",
       "      <th>last_price</th>\n",
       "      <th>previous_year_price</th>\n",
       "      <th>NYSE_NASDAQ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MMM</td>\n",
       "      <td>0.4571</td>\n",
       "      <td>37.66730402</td>\n",
       "      <td>10.519554</td>\n",
       "      <td>25.3366</td>\n",
       "      <td>2.959</td>\n",
       "      <td>142.6619</td>\n",
       "      <td>1.01</td>\n",
       "      <td>199.15</td>\n",
       "      <td>236.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WUBA</td>\n",
       "      <td>0.9377</td>\n",
       "      <td>7.842562916</td>\n",
       "      <td>3.466236</td>\n",
       "      <td>247.7696</td>\n",
       "      <td>1.584</td>\n",
       "      <td>4.1318</td>\n",
       "      <td>1.18</td>\n",
       "      <td>69.68</td>\n",
       "      <td>68.93</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EGHT</td>\n",
       "      <td>4.0372</td>\n",
       "      <td>-41.19122937</td>\n",
       "      <td>7.989302</td>\n",
       "      <td>31.7871</td>\n",
       "      <td>2.115</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>19.775</td>\n",
       "      <td>14.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AHC</td>\n",
       "      <td>1.6529</td>\n",
       "      <td>12.38072109</td>\n",
       "      <td>1.109514</td>\n",
       "      <td>971.1333</td>\n",
       "      <td>1.213</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>4.75</td>\n",
       "      <td>4.85</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAC</td>\n",
       "      <td>0.3138</td>\n",
       "      <td>-12.45305726</td>\n",
       "      <td>1.540616</td>\n",
       "      <td>12.3618</td>\n",
       "      <td>14.426</td>\n",
       "      <td>193.2304</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>9.86</td>\n",
       "      <td>9.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol  cash_ratio return_to_equity  price_to_book        pe  \\\n",
       "0    MMM      0.4571      37.66730402      10.519554   25.3366   \n",
       "1   WUBA      0.9377      7.842562916       3.466236  247.7696   \n",
       "2   EGHT      4.0372     -41.19122937       7.989302   31.7871   \n",
       "3    AHC      1.6529      12.38072109       1.109514  971.1333   \n",
       "4    AAC      0.3138     -12.45305726       1.540616   12.3618   \n",
       "\n",
       "   short_interest_ratio  debt_to_equity   eps last_price  previous_year_price  \\\n",
       "0                 2.959        142.6619  1.01     199.15               236.58   \n",
       "1                 1.584          4.1318  1.18      69.68                68.93   \n",
       "2                 2.115          0.0000 -0.14     19.775                14.10   \n",
       "3                 1.213          0.0000 -0.19       4.75                 4.85   \n",
       "4                14.426        193.2304 -0.01       9.86                 9.11   \n",
       "\n",
       "   NYSE_NASDAQ  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./resources/2018.csv\")\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picking the financial ratios was part of the feature engineering that we had to do for this project. As one might know, there are a number of financial ratios that can describe the fundamental state of company. They usually contain information about the health/growth/profitability of the company, which are all important factors for the company and if an investor feels it is good enough to invest in.\n",
    "\n",
    "We chose the ratios based on the 2 following factors:\n",
    "    1) Popularity (among investors)\n",
    "    2) Availability from our data source (Bloomberg)\n",
    "    \n",
    "As far as 1) goes, the ratios we picked were favourites among fundamental investors. They have been known to be used for fundamental analysis for many years and seem to describe a company and it's future performance well. There are a number of other factors/ratios that we wanted to use, to further the analysis, however our data vendor either kept running out of data, or there was not enought coverage of those ratios for the companies we selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Outperforming and Underperfoming the market \n",
    "The market return is defined as the 1 year log return of the market. Stock is labelled \"outperform\" or \"1\" when its 1 year log return is bigger than its respective market return(NYSE or NASDAQ). Stock is labelled \"underperform\" or \"0\" vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nyse_previous_close = 12412.07\n",
    "nyse_one_year_close = 11812.20\n",
    "\n",
    "nasdaq_previous_close = 7712.9502\n",
    "nasdaq_one_year_close = 6233.9502\n",
    "\n",
    "nasdaq_begin_index = 977\n",
    "\n",
    "nyse_performance = math.log(nyse_previous_close/nyse_one_year_close)\n",
    "nasdaq_performance = math.log(nasdaq_previous_close/nasdaq_one_year_close)\n",
    "\n",
    "# print(nyse_performance)\n",
    "\n",
    "df = df.infer_objects()\n",
    "df['last_price'] = df['last_price'].astype('float')\n",
    "\n",
    "#print(df.dtypes)\n",
    "\n",
    "df['performance'] = np.log(df['last_price'] /df['previous_year_price'])\n",
    "df['status'] = 0\n",
    "nyse_performance = (df.loc[range(0,nasdaq_begin_index),'performance']>nyse_performance).astype('int')\n",
    "nasdaq_performance = (df.loc[range(nasdaq_begin_index,len(df)),'performance']>nasdaq_performance).astype('int')\n",
    "df['status'] = nyse_performance\n",
    "df.loc[nasdaq_begin_index:len(df),'status'] = nasdaq_performance\n",
    "df['status'] = df['status'].astype('int')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alex write normalisation definition\n",
    "The features are normalised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_data(x,min,max):\n",
    "    return (x-min)/(max-min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#normalize data\n",
    "df[['cash_ratio','return_to_equity','price_to_book','pe','short_interest_ratio','debt_to_equity','eps']] = normalize(df[['cash_ratio','return_to_equity','price_to_book','pe','short_interest_ratio','debt_to_equity','eps']],axis=0, norm='max')\n",
    "df.dropna()     # need to dropnan else can't model\n",
    "\n",
    "\n",
    "cr = normalize_data(df['cash_ratio'],min(df['cash_ratio']),max(df['cash_ratio']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alex change here K-fold for spiltting data \n",
    "K-fold is performed to split the dataset into train, val and test. Integer 1 to 10 is uniform randomly assigned to each stock. Stocks with \"fold\" number less or equal to 6.5 is splitted to the train data set. Stocks with \"fold\" number bigger than 6.5  and less or equal to 9 is splitted to the validation data set. The rest is splitted to the test data set. We drop Nan again to make sure there are no missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set random seed for folding\n",
    "np.random.seed(2018)\n",
    "df['fold'] = np.random.uniform(1,10,len(df))\n",
    "\n",
    "id_train = np.where(df['fold']<=6.5)[0]\n",
    "id_val = np.intersect1d(np.where(df['fold'] > 6.5)[0],np.where(df['fold'] <= 9)[0])\n",
    "id_test = np.where(df['fold']>9)[0]\n",
    "\n",
    "n_train = len(id_train)\n",
    "n_val = len(id_val)\n",
    "n_test = len(id_test)\n",
    "\n",
    "\n",
    "\n",
    "X = df[['cash_ratio','return_to_equity','price_to_book',\n",
    "        'pe','short_interest_ratio','debt_to_equity','eps',\n",
    "        'NYSE_NASDAQ']]\n",
    "y = df['status']\n",
    "\n",
    "\n",
    "## Prepare train data set, droping Nan\n",
    "X_train = X.iloc[id_train]\n",
    "X_train = X_train.dropna()\n",
    "\n",
    "\n",
    "y_train = y.iloc[id_train]\n",
    "y_train = y_train.dropna()\n",
    "\n",
    "\n",
    "\n",
    "## Prepare validation data set\n",
    "X_val = X.iloc[id_val]\n",
    "X_val = X_val.dropna()\n",
    "\n",
    "\n",
    "y_val_true = y.iloc[id_val]\n",
    "y_val_true = y_val_true.dropna()\n",
    "\n",
    "\n",
    "\n",
    "## Prepare test data set, droping Nan\n",
    "X_test = X.iloc[id_test]\n",
    "X_test = X_test.dropna()\n",
    "\n",
    "y_test_true = y.iloc[id_test]\n",
    "y_test_true = y_test_true.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is not sequential/ time-series, there is no need of forward chaining. However the release of the financial ratios might not matches the return (whether the stock return outperform/ underperform the market) info at the exact same date. It is very difficult to pinpoint a date where all price info and financial ratios are available. We accept the shortcoming and proceed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics\n",
    "Assuming we are \"the experts\" of a long only fund. We would like to minimise our number of bad choices and maximise our number of good choices, \"False Positive\" and \"True Positive\" (since we don't short underperforming stocks). We uses precision as our performance metric. \n",
    "\n",
    "$Precision = \\frac{True \\ Positive}{True  \\ Positive + False Positive}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we calculate the confusion matrix for each model:  \n",
    "    $ \\ A  = \\begin{pmatrix} \n",
    "True \\ Negative & False \\ Positive \\\\\n",
    "False  \\ Negative & True \\ Positive\n",
    "\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def confusion_matrix_for_reg(predict, real):\n",
    "    \n",
    "    ## when running regression, we expect the predict performance would not consists only 1 or 0. \n",
    "    ## We round the predicted performance to the first decimal\n",
    "    predict = np.array(np.round(predict))\n",
    "    \n",
    "    ## We also limit the prediction between 0 and 1\n",
    "    predict = predict.clip(min=0, max=1)\n",
    "    \n",
    "    real = np.array(real)\n",
    "\n",
    "\n",
    "    ## use AND gate to check for TP\n",
    "    TP = np.sum(np.logical_and(predict, real))\n",
    "    ## sum of all 1s in predict - TP = FP\n",
    "    FP = np.sum(predict ==1) - TP\n",
    "\n",
    "\n",
    "    ## first flip predict and real 1 and 0s, and use the same AND gate to check for TN\n",
    "    TN = np.sum(np.logical_and(np.logical_not(predict), np.logical_not(real)))\n",
    "    FN = np.sum(predict ==0) - TN\n",
    "\n",
    "\n",
    "\n",
    "    conf_matrix = [[TN, FP], [FN, TP]]\n",
    "\n",
    "\n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monkey stock picking\n",
    "We simulate the monkey stock picking by randomly choosing under or overperform for each stock. This is our first naive method as stocks are simply picked by random, diregarding their fundamental financial ratios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(2018)\n",
    "y_train_from_random = np.random.uniform(1,0,len(y_train))\n",
    "\n",
    "y_test_from_random = np.random.uniform(1,0,len(y_test_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS regression \n",
    "One of the textbook methods of fundamental analysis is a simple OLS regression(fundamental model) to model stock performance. OLS can be seen as our second naive method as we assume there the relationships between features and the stock performance are linear. The loss function to be minimised is\n",
    "\n",
    "$arg \\ min_{\\beta} ||y - X  \\ \\beta \\ ||_{2}^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y is the labelled data, 1 and 0 representing performance in our case. X represents the matrix of features with $\\beta$ as the regression coefficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model is fitted with the train data set to predict train performance. The second model is first fitted with validation data set and used to predict the test performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_OLS(data_X_train, data_y_train, data_X_val, data_y_val_true, data_X_test):\n",
    "    linreg = linear_model.LinearRegression(fit_intercept=True)  # can change with or without intercept\n",
    "    linreg.fit(data_X_train, data_y_train)\n",
    "    linreg.get_params()\n",
    "    y_train_from_OLS = linreg.predict(data_X_train)\n",
    "\n",
    "    linreg.fit(data_X_val,data_y_val_true)\n",
    "    linreg.get_params()\n",
    "    y_test_from_OLS = linreg.predict(data_X_test)\n",
    "    return y_train_from_OLS, y_test_from_OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO regression\n",
    "The LASSO regression is a penalised regression where \n",
    "\n",
    "$arg \\ min_{\\beta} ||y - X  \\ \\beta \\ ||_{2}^2 + \\lambda \\ || \\beta ||_1$\n",
    "\n",
    "is the loss function to be minimised. The first term is to reduce bias and the second term is to reduce varaince. $\\lambda$ is the penalised coefficient for increased complexity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_Lassi(data_X_train, data_y_train, a, data_X_val):\n",
    "    Lassi = Lasso(alpha=a, tol=1e-5)\n",
    "    Lassi.fit(data_X_train, data_y_train)\n",
    "    y_train_from_LASSO = Lassi.predict(data_X_train)\n",
    "\n",
    "    y_val_from_LASSO = Lassi.predict(data_X_val)\n",
    "\n",
    "    return y_train_from_LASSO, y_val_from_LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression\n",
    "\n",
    "The Ridge regression is a penalised regression where \n",
    "\n",
    "$arg \\ min_{\\beta} ||y - X  \\ \\beta \\ ||_{2}^2 + \\lambda \\ || \\beta ||^2_{2}$\n",
    "\n",
    "is to be minimised. The difference between LASSO and Ridge is the penality term for complexity is squared insetad of absoluted in Ridge. LASSO's regularisation is L1 instead of L2 regularisation in Ridge. This allows LASSO to shrinks the irrelevant features to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_Rachel(data_X_train, data_y_train, a, data_X_val):\n",
    "    Rachel = Ridge(alpha=a, tol=1e-5)\n",
    "    Rachel.fit(data_X_train, data_y_train)\n",
    "    y_train_from_Ridge = Rachel.predict(data_X_train)\n",
    "\n",
    "    y_val_from_Ridge = Rachel.predict(data_X_val)\n",
    "\n",
    "    return y_train_from_Ridge, y_val_from_Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run OLS regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_from_OLS, y_test_from_OLS = model_OLS(X_train, y_train, X_val, y_val_true, X_test)\n",
    "\n",
    "conf_matrix_train_from_OLS = confusion_matrix_for_reg(y_train_from_OLS, y_train)\n",
    "\n",
    "conf_matrix_test_from_OLS = confusion_matrix_for_reg(y_test_from_OLS, y_test_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimising lambda for the penalised regression models\n",
    "The amount of penality with increased compexity is adjusted by $\\lambda$. The optimal $\\lambda$ is chose based on the the precision using a for loop fitting the validation data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kingf.wong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/kingf.wong/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/kingf.wong/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/kingf.wong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:33: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/kingf.wong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2857142857142857, 0.33333333333333331, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "## TODO Optimizing LASSO and Ridge Lambda\n",
    "\n",
    "## initialising\n",
    "\n",
    "# range of lambda we are testing\n",
    "x_axis_interval = np.arange(0,0.7, 1e-3)\n",
    "\n",
    "# list of confusion matrix and precision\n",
    "conf_matrix_train_from_LASSO = []\n",
    "prec_train_LASSO =[]\n",
    "\n",
    "conf_matrix_val_from_LASSO = []\n",
    "prec_val_LASSO =[]\n",
    "\n",
    "conf_matrix_train_from_Ridge = []\n",
    "prec_train_Ridge =[]\n",
    "\n",
    "conf_matrix_val_from_Ridge =[]\n",
    "prec_val_Ridge = []\n",
    "\n",
    "\n",
    "## finding optimal lambda for the least amount of False Positive\n",
    "for a in x_axis_interval:\n",
    "\n",
    "    y_train_from_LASSO, y_val_from_LASSO = model_Lassi(X_train, y_train, a, X_val)\n",
    "    \n",
    "    ## calculate the confusion matrix\n",
    "    conf_matrix_train_from_LASSO.append(confusion_matrix_for_reg(y_train_from_LASSO, y_train))\n",
    "    \n",
    "    ## calculating the precision by TP/ (TP + FP)\n",
    "    prec_train_LASSO.append(confusion_matrix_for_reg(y_train_from_LASSO, y_train)[1][1] /\n",
    "                            (confusion_matrix_for_reg(y_train_from_LASSO, y_train)[1][1] +\n",
    "                             confusion_matrix_for_reg(y_train_from_LASSO, y_train)[0][1]))\n",
    "\n",
    "\n",
    "    conf_matrix_val_from_LASSO.append(confusion_matrix_for_reg(y_val_from_LASSO, y_val_true))\n",
    "    prec_val_LASSO.append(confusion_matrix_for_reg(y_val_from_LASSO, y_val_true)[1][1] /\n",
    "                          (confusion_matrix_for_reg(y_val_from_LASSO, y_val_true)[1][1] +\n",
    "                           confusion_matrix_for_reg(y_val_from_LASSO, y_val_true)[0][1]))\n",
    "\n",
    "    y_train_from_Ridge, y_val_from_Ridge = model_Rachel(X_train, y_train, a, X_val)\n",
    "\n",
    "    conf_matrix_train_from_Ridge.append(confusion_matrix_for_reg(y_train_from_Ridge, y_train))\n",
    "    \n",
    "    prec_train_Ridge.append(confusion_matrix_for_reg(y_train_from_Ridge, y_train)[1][1] /\n",
    "                            (confusion_matrix_for_reg(y_train_from_Ridge, y_train)[1][1] +\n",
    "                             confusion_matrix_for_reg(y_train_from_Ridge, y_train)[0][1]))\n",
    "\n",
    "    conf_matrix_val_from_Ridge.append(confusion_matrix_for_reg(y_val_from_Ridge, y_val_true))\n",
    "    \n",
    "    prec_val_Ridge.append(confusion_matrix_for_reg(y_val_from_Ridge, y_val_true)[1][1] /\n",
    "                          (confusion_matrix_for_reg(y_train_from_Ridge, y_train)[1][1] +\n",
    "                           confusion_matrix_for_reg(y_train_from_Ridge, y_train)[0][1]))\n",
    "\n",
    "print(prec_val_Ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision across lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1176b8cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the precisions across lambdas\n",
    "plt.scatter(x_axis_interval,prec_train_LASSO, label=\"LASSO train\", s=0.3)\n",
    "plt.scatter(x_axis_interval, prec_val_LASSO, label=\"LASSO val\", s=0.3)\n",
    "plt.scatter(x_axis_interval,prec_train_Ridge, label=\"Ridge train\", s=0.3)\n",
    "plt.scatter(x_axis_interval,prec_val_Ridge, label=\"Ridge val\", s=0.3)\n",
    "\n",
    "plt.xlabel('tuning parameter lambda')\n",
    "plt.ylabel('precision of train and val data fitting')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal model with optimal lambda for predicting from test data set\n",
    "We chose the optimal $\\lambda$ according to the precisison of the model. We retrain the model with the validation data set and the optimal $\\lambda$ to predict using test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kingf.wong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/kingf.wong/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/kingf.wong/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "a_range = x_axis_interval\n",
    "\n",
    "## Only optimize for minimum FP, long only fund\n",
    "opt_lambda_LASSO = a_range[prec_val_LASSO.index(max(prec_val_LASSO))]\n",
    "opt_lambda_Ridge = a_range[prec_val_Ridge.index(max(prec_val_Ridge))]\n",
    "\n",
    "## predicting test data set with optimal lambda/ alpha, returning confusion matrix\n",
    "\n",
    "Lassi_opt = Lasso(alpha=opt_lambda_LASSO, tol=1e-5)\n",
    "Lassi_opt.fit(X_val, y_val_true)\n",
    "y_test_from_Lassi = Lassi_opt.predict(X_test)\n",
    "\n",
    "conf_matrix_test_Lassi = confusion_matrix_for_reg(y_test_from_Lassi,y_test_true)\n",
    "\n",
    "\n",
    "Rachel_opt = Ridge(alpha=opt_lambda_Ridge, tol=1e-5)\n",
    "Rachel_opt.fit(X_val, y_val_true)\n",
    "y_test_from_Rachel = Rachel_opt.predict(X_test)\n",
    "\n",
    "conf_matrix_test_Rachel = confusion_matrix_for_reg(y_test_from_Rachel, y_test_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal lambda/ tuning parameters for LASSO: 0.0 \n",
      " optimal lambda/ tuning parameters for Ridge: 0.002\n",
      "OLS's confusion matrix of training: [[650, 4], [446, 3]] \n",
      " OLS's confusion matrix of testing: [[121, 6], [83, 7]] \n",
      " LASSO's confusion matrix of testing [[121, 6], [83, 7]] \n",
      " Ridge's confusion matrix of testing [[123, 4], [87, 3]]\n",
      "train_from_random test_from_random train_from_OLS test_from_OLS test_LASSO test_Ridge\n",
      "[0.4177215189873418, 0.4215686274509804, 0.42857142857142855, 0.5384615384615384, 0.5384615384615384, 0.42857142857142855]\n"
     ]
    }
   ],
   "source": [
    "print(\"optimal lambda/ tuning parameters for LASSO:\", opt_lambda_LASSO,\"\\n\",\"optimal lambda/ tuning parameters for Ridge:\", opt_lambda_Ridge)\n",
    "\n",
    "\n",
    "print(\"OLS's confusion matrix of training:\", conf_matrix_train_from_OLS,\"\\n\",\n",
    "      \"OLS's confusion matrix of testing:\", conf_matrix_test_from_OLS,\"\\n\",\n",
    "      \"LASSO's confusion matrix of testing\", conf_matrix_test_Lassi,\"\\n\",\n",
    "      \"Ridge's confusion matrix of testing\", conf_matrix_test_Rachel)\n",
    "\n",
    "\n",
    "## TODO Performance metric, calculating precision\n",
    "\n",
    "## first get confusion matrix of randomly picking\n",
    "\n",
    "conf_matrix_train_from_random = confusion_matrix_for_reg(y_train_from_random, y_train)\n",
    "conf_matrix_test_from_random = confusion_matrix_for_reg(y_test_from_random, y_test_true)\n",
    "\n",
    "conf_matrix_list = [conf_matrix_train_from_random, conf_matrix_test_from_random,\n",
    "                    conf_matrix_train_from_OLS, conf_matrix_test_from_OLS,\n",
    "                    conf_matrix_test_Lassi, conf_matrix_test_Rachel]\n",
    "\n",
    "\n",
    "prec_list = []\n",
    "\n",
    "for i in conf_matrix_list:\n",
    "    prec_list.append(float(i[1][1])/ float(i[0][1] + i[1][1]))\n",
    "print(\"train_from_random\", \"test_from_random\",\n",
    "                    \"train_from_OLS\", \"test_from_OLS\",\n",
    "                    \"test_LASSO\", \"test_Ridge\")\n",
    "print(prec_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It seems the optimal lambda is exactly 0 for LASSO and almost 0 for Ridge regression, converging back to an OLS regression. Indicating there is no advantage to increase the model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM: Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM is a machine learning technique most often utilized for classification. The goal of the learning technique is to draw a hyperplane(in linear 2D SVM that is a line) between data that is distinctly classified in different categories. The best choice will be the hyperplane that leaves the maximum margin from both classes. The goal of the svm algorithm is to maximize the separability between the data points and the hyperplane.\n",
    "\n",
    "Any hyperplane can be written as $w \\cdot x - b = 0$, and having a binary classification of -1,1 yields the following 2 equations $w \\cdot x - b = -1$ and $w \\cdot x - b = 1$. Geometrically, the distance between these 2 lines (margin) is $\\frac{2}{||w||}$. The job of the classifier is to minimize the fraction with the constraints of correct classification. More conretely, the minimization that needs to take place is $\\min{w}||w||^2$ with the constraint of $y_i(w^Tx_i+b) >= 1$\n",
    "\n",
    "In general, there is a trade off between the margin and the number of mistakes on the data i.e. the points can be linearly separated but there is a narrow margin or the margin solution is better even though some constraints might be violated. As a result, slack variables can be introduced ($\\xi$), which would control the aforementioned effect. Thus we reach a \"soft\" margin solution where the optimization becomes $\\min{w}||w||^2 + C\\sum_{i}^{N}\\xi_i$ subject to $y_i(w^Tx_i+b) >= 1-\\xi_i$, where C is a regularization parameter.\n",
    "\n",
    "Rewriting the the constraint $y_i(w^Tx_i+b) >= 1-\\xi_i$ as $y_if(x_i)>=1-\\xi_i$ and with $\\xi_i>0$ we get $\\xi_i=max(0,1-y_if(x_i)$, which makes our problem an uncostrainted optimization over w of the form: $\\min||w||^2+C\\sum_{i}^{N}\\max(0,1-y_if(\\xi_i))$ with the second part of the equation being the loss function.\n",
    "\n",
    "\n",
    "Our problem is perfect for a SVM as it's main function is to classify companies as underperforming or overperforming. Minimizing the loss function would create a hyperplane that would distinguish between overperforming and underperforming companies. \n",
    "\n",
    "In terms of hyperparameters, scikitlearn svm has 2 main ones. The C and the gamma. Following https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf, we set the range of the hyperparameters accordingly. Finally, since it is hard to visualize what our dataset would look like, we added another hyperparameter which was the kernel of the svm. We excluded the polynomial kernel, as it took a very long time to train and fit the data.\n",
    "\n",
    "The metrics that were used for the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
